{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e7bc164",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c984a61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jgduarte/Documents/RA/Projects/3M/PolyGraphPy/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GraphUNet\n",
    "from diffusers import DDPMScheduler\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import QED\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "# Allowed atoms and bonds\n",
    "allowed_atoms = [1, 6, 7, 8, 9, 11, 15, 16, 17, 19, 30, 35, 53]\n",
    "allowed_bonds = [0, 1, 2, 3]  # Include 0 for no bond\n",
    "atom_type_to_idx = {z: i for i, z in enumerate(allowed_atoms)}\n",
    "\n",
    "# Dataset\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, csv_file):\n",
    "        self.df = pd.read_csv(csv_file)\n",
    "        self.df = self.df[self.df['chain_size'] == 0]\n",
    "        self.df = self.df[self.df['static_polarizability'] <= 217]\n",
    "        scaler = StandardScaler()\n",
    "        self.df['scaled_polarizability'] = scaler.fit_transform(self.df[['static_polarizability']])\n",
    "        self.atom_type_to_idx = atom_type_to_idx\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        smiles = row['smiles']\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        num_atoms = mol.GetNumAtoms()\n",
    "\n",
    "        x = torch.zeros(num_atoms, len(self.atom_type_to_idx) + 2)  # atom types + valence + degree\n",
    "        target_node = torch.full((num_atoms,), -100, dtype=torch.long)  # ignore_index for loss\n",
    "        for i, atom in enumerate(mol.GetAtoms()):\n",
    "            z = atom.GetAtomicNum()\n",
    "            if z not in self.atom_type_to_idx:\n",
    "                continue  # skip unknown atoms, or handle differently\n",
    "            idx_atom = self.atom_type_to_idx[z]\n",
    "            x[i, idx_atom] = 1\n",
    "            x[i, -1] = atom.GetTotalValence()\n",
    "            x[i, -2] = mol.GetAtomWithIdx(i).GetDegree()\n",
    "            target_node[i] = idx_atom\n",
    "\n",
    "        edge_index = []\n",
    "        target_edge = []\n",
    "        # True bonds\n",
    "        for bond in mol.GetBonds():\n",
    "            i, j = bond.GetBeginAtomIdx(), bond.GetEndAtomIdx()\n",
    "            bt = int(bond.GetBondTypeAsDouble())\n",
    "            if bt in allowed_bonds[1:]:\n",
    "                idx = allowed_bonds.index(bt)\n",
    "                edge_index.extend([[i, j], [j, i]])\n",
    "                target_edge.extend([idx, idx])\n",
    "        # Sample non-bonded pairs\n",
    "        non_bond_pairs = torch.combinations(torch.arange(num_atoms), r=2)\n",
    "        num_non_bonds = len(edge_index)  # e.g., 2x true bonds\n",
    "        if len(non_bond_pairs) > num_non_bonds:\n",
    "            indices = torch.randperm(len(non_bond_pairs))[:num_non_bonds]\n",
    "            non_bond_pairs = non_bond_pairs[indices]\n",
    "        else:\n",
    "            non_bond_pairs = non_bond_pairs\n",
    "        edge_index.extend(non_bond_pairs.tolist())\n",
    "        edge_index.extend(non_bond_pairs.flip(1).tolist())\n",
    "        target_edge.extend([0] * 2 * len(non_bond_pairs))\n",
    "        edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "        target_edge = torch.tensor(target_edge, dtype=torch.long)\n",
    "        edge_attr = torch.tensor([allowed_bonds[te] for te in target_edge], dtype=torch.float).unsqueeze(-1)\n",
    "        graph = Data(x=x, edge_index=edge_index, edge_attr=edge_attr, num_nodes=num_atoms)\n",
    "        label = row['scaled_polarizability']\n",
    "\n",
    "        return graph, torch.tensor(label, dtype=torch.float32), target_node, target_edge\n",
    "\n",
    "# Model\n",
    "class ClassConditionedGNN(nn.Module):\n",
    "    def __init__(self, class_emb_size=75):\n",
    "        super().__init__()\n",
    "        in_channels = len(allowed_atoms) + 2\n",
    "        self.class_emb = nn.Linear(1, class_emb_size)\n",
    "        self.time_emb = nn.Embedding(1000, 64)\n",
    "        self.node_model = GraphUNet(\n",
    "            in_channels=in_channels + class_emb_size + 64,\n",
    "            hidden_channels=256,\n",
    "            out_channels=len(allowed_atoms),  # node classification logits\n",
    "            depth=4\n",
    "        )\n",
    "        self.edge_model = nn.Sequential(\n",
    "            nn.Linear(2 * (in_channels + class_emb_size + 64) + 1, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128, len(allowed_bonds))  # Now 4\n",
    "        )\n",
    "\n",
    "    def forward(self, graph, t, class_labels):\n",
    "        if hasattr(graph, 'num_graphs'):\n",
    "            batch_idx = graph.batch\n",
    "        else:\n",
    "            batch_idx = torch.zeros(graph.num_nodes, dtype=torch.long, device=graph.x.device)\n",
    "\n",
    "        class_cond = self.class_emb(class_labels.unsqueeze(-1))\n",
    "        time_cond = self.time_emb(t)\n",
    "        class_cond = class_cond[batch_idx]\n",
    "        time_cond = time_cond[batch_idx]\n",
    "\n",
    "        node_input = torch.cat((graph.x, class_cond, time_cond), dim=-1)\n",
    "        pred_node_logits = self.node_model(node_input, graph.edge_index)\n",
    "\n",
    "        u, v = graph.edge_index\n",
    "        edge_input = torch.cat((node_input[u], node_input[v], graph.edge_attr), dim=-1)\n",
    "        pred_edge_logits = self.edge_model(edge_input)\n",
    "\n",
    "        return {'x': pred_node_logits, 'edge_attr': pred_edge_logits}\n",
    "\n",
    "class GraphDDPMScheduler(DDPMScheduler):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.alphas_cumprod = self.alphas_cumprod.to(device)\n",
    "\n",
    "    def add_noise(self, original_samples, noise, timesteps):\n",
    "        noisy = original_samples.clone()\n",
    "\n",
    "        # Handle single graph vs batch of graphs\n",
    "        if hasattr(original_samples, 'batch') and original_samples.batch is not None:\n",
    "            batch_idx = original_samples.batch\n",
    "        else:\n",
    "            batch_idx = torch.zeros(original_samples.num_nodes, dtype=torch.long, device=original_samples.x.device)\n",
    "\n",
    "        # Get scaling coefficients\n",
    "        sqrt_alpha_prod = self.alphas_cumprod[timesteps][batch_idx].sqrt().unsqueeze(-1)\n",
    "        sqrt_one_minus_alpha_prod = (1 - self.alphas_cumprod[timesteps][batch_idx]).sqrt().unsqueeze(-1)\n",
    "\n",
    "        # Add noise to atom type one-hot (not valence/degree)\n",
    "        noisy.x[:, :len(allowed_atoms)] = sqrt_alpha_prod * original_samples.x[:, :len(allowed_atoms)] + \\\n",
    "                                        sqrt_one_minus_alpha_prod * noise['x']\n",
    "        noisy.x[:, len(allowed_atoms):] = original_samples.x[:, len(allowed_atoms):]\n",
    "\n",
    "        # Edge noise (need edge_batch)\n",
    "        edge_index = original_samples.edge_index\n",
    "        edge_batch = batch_idx[edge_index[0]]  # shape [E]\n",
    "        sqrt_alpha_prod_edge = self.alphas_cumprod[timesteps][edge_batch].sqrt().unsqueeze(-1)\n",
    "        sqrt_one_minus_alpha_prod_edge = (1 - self.alphas_cumprod[timesteps][edge_batch]).sqrt().unsqueeze(-1)\n",
    "\n",
    "        noisy.edge_attr = sqrt_alpha_prod_edge * original_samples.edge_attr + \\\n",
    "                        sqrt_one_minus_alpha_prod_edge * noise['edge_attr']\n",
    "\n",
    "        return noisy\n",
    "\n",
    "    def step(self, model_output, timestep, sample):\n",
    "        t = timestep.item() if timestep.dim() == 0 else timestep[0].item()\n",
    "        prev_t = t - 1\n",
    "        alpha_prod_t = self.alphas_cumprod[t]\n",
    "        alpha_prod_t_prev = self.alphas_cumprod[prev_t] if prev_t >= 0 else 1.0\n",
    "        beta_prod_t = 1 - alpha_prod_t\n",
    "        beta_prod_t_prev = 1 - alpha_prod_t_prev\n",
    "        alpha_t = self.alphas[t]\n",
    "        # Node\n",
    "        x0_atom = torch.softmax(model_output['x'], dim=-1)\n",
    "        x0_x = torch.cat((x0_atom, sample.x[:, -2:]), dim=-1)\n",
    "        # Edge (expected bond order as scalar)\n",
    "        bond_probs = torch.softmax(model_output['edge_attr'], dim=-1)\n",
    "        x0_edge = (bond_probs * torch.tensor(allowed_bonds, device=sample.x.device)).sum(dim=-1).unsqueeze(-1)\n",
    "        if prev_t < 0:\n",
    "            return Data(x=x0_x, edge_attr=x0_edge, edge_index=sample.edge_index, num_nodes=sample.num_nodes)\n",
    "        # Mean\n",
    "        coef1 = alpha_prod_t_prev ** 0.5 * alpha_t / beta_prod_t\n",
    "        coef2 = (alpha_prod_t_prev / alpha_prod_t) ** 0.5 * (1 - alpha_t) / beta_prod_t\n",
    "        mu_x = coef1 * x0_x + coef2 * sample.x\n",
    "        mu_edge = coef1 * x0_edge + coef2 * sample.edge_attr\n",
    "        # Variance\n",
    "        variance = beta_prod_t_prev * (1 - alpha_t) / beta_prod_t\n",
    "        sigma = variance ** 0.5\n",
    "        prev_x = mu_x + sigma * torch.randn_like(sample.x)\n",
    "        prev_edge = mu_edge + sigma * torch.randn_like(sample.edge_attr)\n",
    "        return Data(x=prev_x, edge_attr=prev_edge, edge_index=sample.edge_index, num_nodes=sample.num_nodes)\n",
    "\n",
    "# Valence limits for atoms you have (extend as needed)\n",
    "VALENCE_LIMITS = {\n",
    "    1: 1, 6: 4, 7: 3, 8: 2, 9: 1, 11: 1, 15: 3, 16: 2, 17: 1, 19: 1, 30: 2, 35: 1, 53: 1\n",
    "}\n",
    "\n",
    "def valence_violation_loss(graph, pred_node_logits, pred_edge_logits):\n",
    "    # node: [N, num_atom_types], edge: [E, num_bond_types]\n",
    "    atom_probs = torch.softmax(pred_node_logits, dim=-1)\n",
    "    bond_probs = torch.softmax(pred_edge_logits, dim=-1)\n",
    "\n",
    "    inv_atom_map = {v: k for k, v in atom_type_to_idx.items()}\n",
    "    expected_atomic_nums = torch.zeros(graph.num_nodes, device=graph.x.device)\n",
    "    for idx in range(len(atom_type_to_idx)):\n",
    "        expected_atomic_nums += atom_probs[:, idx] * inv_atom_map[idx]\n",
    "\n",
    "    expected_bond_orders = (bond_probs * torch.tensor(allowed_bonds, device=graph.x.device)).sum(dim=-1)\n",
    "    bond_sum_per_node = torch.zeros(graph.num_nodes, device=graph.x.device)\n",
    "    for idx, (u, v) in enumerate(graph.edge_index.t()):\n",
    "        bond_sum_per_node[u] += expected_bond_orders[idx]\n",
    "        bond_sum_per_node[v] += expected_bond_orders[idx]\n",
    "\n",
    "    valence_limits_tensor = torch.tensor(\n",
    "        [VALENCE_LIMITS.get(z.item(), 4) for z in expected_atomic_nums.round().int()],\n",
    "        device=graph.x.device, dtype=torch.float\n",
    "    )\n",
    "    violation = torch.relu(bond_sum_per_node - valence_limits_tensor)\n",
    "    return violation.mean()\n",
    "\n",
    "# Convert graph to mol (updated to use allowed atoms)\n",
    "def graph_to_mol(data: Data):\n",
    "    mol = Chem.RWMol()\n",
    "    node_map = {}\n",
    "\n",
    "    for i, node_feat in enumerate(data.x):\n",
    "        atomic_type_idx = node_feat[:len(allowed_atoms)].argmax().item()  # Only on atom part\n",
    "        if atomic_type_idx >= len(allowed_atoms):\n",
    "            return None\n",
    "        atomic_num = allowed_atoms[atomic_type_idx]\n",
    "        atom = Chem.Atom(atomic_num)\n",
    "        idx = mol.AddAtom(atom)\n",
    "        node_map[i] = idx\n",
    "\n",
    "    added_bonds = set()\n",
    "    for k in range(data.edge_index.size(1)):\n",
    "        u = data.edge_index[0, k].item()\n",
    "        v = data.edge_index[1, k].item()\n",
    "        if u >= v or (u, v) in added_bonds:\n",
    "            continue\n",
    "        added_bonds.add((u, v))\n",
    "        bond_val = data.edge_attr[k].item()  # scalar\n",
    "        bond_type_idx = round(bond_val)\n",
    "        if bond_type_idx <= 0 or bond_type_idx >= len(allowed_bonds):\n",
    "            continue\n",
    "        bond_type_val = allowed_bonds[bond_type_idx]\n",
    "        if bond_type_val == 1:\n",
    "            bond_type = Chem.rdchem.BondType.SINGLE\n",
    "        elif bond_type_val == 2:\n",
    "            bond_type = Chem.rdchem.BondType.DOUBLE\n",
    "        elif bond_type_val == 3:\n",
    "            bond_type = Chem.rdchem.BondType.TRIPLE\n",
    "        else:\n",
    "            continue\n",
    "        try:\n",
    "            mol.AddBond(node_map[u], node_map[v], bond_type)\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "    try:\n",
    "        mol = mol.GetMol()\n",
    "        Chem.SanitizeMol(mol)\n",
    "        return mol\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def is_valid_molecule(mol, min_atoms=3, max_atoms=50, min_qed=0.2):\n",
    "    if mol is None:\n",
    "        return False\n",
    "    if mol.GetNumAtoms() < min_atoms or mol.GetNumAtoms() > max_atoms:\n",
    "        return False\n",
    "    try:\n",
    "        qed_score = QED.qed(mol)\n",
    "        if qed_score < min_qed:\n",
    "            return False\n",
    "    except:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "from torch_geometric.data import Batch\n",
    "\n",
    "def custom_collate(batch):\n",
    "    graphs, labels, target_nodes, target_edges = zip(*batch)\n",
    "    \n",
    "    # Type and shape sanity checks\n",
    "    for i, (g, tn, te) in enumerate(zip(graphs, target_nodes, target_edges)):\n",
    "        assert isinstance(g, Data), f\"Item {i} graph not Data!\"\n",
    "        assert tn.dim() == 1, f\"Item {i} target_node not 1D!\"\n",
    "        assert te.dim() == 1, f\"Item {i} target_edge not 1D!\"\n",
    "        assert tn.size(0) == g.num_nodes, f\"Item {i} target_node size mismatch!\"\n",
    "        assert te.size(0) == g.edge_index.size(1), f\"Item {i} target_edge size mismatch!\"\n",
    "    \n",
    "    batch_graph = Batch.from_data_list(graphs)\n",
    "    labels = torch.stack(labels)\n",
    "    target_nodes = torch.cat(target_nodes)\n",
    "    target_edges = torch.cat(target_edges)\n",
    "    return batch_graph, labels, target_nodes, target_edges\n",
    "\n",
    "\n",
    "# Initialize dataset, dataloader, model, optimizer, scheduler\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b339d7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CustomDataset('../polygraphpy/data/polarizability_data.csv')\n",
    "train_dataloader = DataLoader(dataset, batch_size=64, shuffle=True, collate_fn=custom_collate, num_workers=0)\n",
    "\n",
    "net = ClassConditionedGNN().to(device)\n",
    "loss_fn_node = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "loss_fn_edge = nn.CrossEntropyLoss()\n",
    "opt = torch.optim.Adam(net.parameters(), lr=1e-4)\n",
    "\n",
    "noise_scheduler = GraphDDPMScheduler(num_train_timesteps=1000, beta_schedule=\"linear\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6981c718",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/107 [00:00<?, ?it/s]/home/jgduarte/Documents/RA/Projects/3M/PolyGraphPy/.venv/lib/python3.13/site-packages/torch_geometric/utils/sparse.py:277: UserWarning: Sparse CSR tensor support is in beta state. If you miss a functionality in the sparse tensor support, please submit a feature request to https://github.com/pytorch/pytorch/issues. (Triggered internally at /pytorch/aten/src/ATen/SparseCsrTensorImpl.cpp:53.)\n",
      "  adj = torch.sparse_csr_tensor(\n",
      "100%|██████████| 107/107 [03:22<00:00,  1.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 avg loss: 9.13511\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 107/107 [03:23<00:00,  1.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 avg loss: 1.94226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 107/107 [03:21<00:00,  1.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 avg loss: 1.81450\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 107/107 [03:21<00:00,  1.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 avg loss: 1.78269\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 107/107 [03:17<00:00,  1.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 avg loss: 1.75671\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 107/107 [03:03<00:00,  1.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 avg loss: 1.73665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 107/107 [02:23<00:00,  1.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 avg loss: 1.71805\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 107/107 [02:06<00:00,  1.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 avg loss: 1.70126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 107/107 [03:00<00:00,  1.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 avg loss: 1.68807\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 107/107 [02:31<00:00,  1.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 avg loss: 1.67521\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 107/107 [02:01<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 avg loss: 1.66416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 107/107 [02:01<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 avg loss: 1.65352\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 107/107 [02:01<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 avg loss: 1.64197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 107/107 [02:01<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 avg loss: 1.63395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 107/107 [02:01<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 avg loss: 1.62478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 107/107 [02:01<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 avg loss: 1.61728\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 107/107 [02:01<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16 avg loss: 1.60888\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 107/107 [02:01<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17 avg loss: 1.60248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 107/107 [02:01<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 avg loss: 1.59710\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 107/107 [02:01<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19 avg loss: 1.59092\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 107/107 [02:01<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 avg loss: 1.58427\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 107/107 [02:01<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21 avg loss: 1.57869\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 107/107 [02:01<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22 avg loss: 1.57423\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 107/107 [02:01<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23 avg loss: 1.56926\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 107/107 [02:01<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24 avg loss: 1.56808\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 107/107 [02:01<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25 avg loss: 1.56399\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 107/107 [02:01<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26 avg loss: 1.56240\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 107/107 [02:01<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27 avg loss: 1.55748\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 107/107 [02:01<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28 avg loss: 1.55319\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 107/107 [02:01<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29 avg loss: 1.55070\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 107/107 [02:01<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30 avg loss: 1.54647\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 107/107 [02:01<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31 avg loss: 1.54915\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 107/107 [02:01<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32 avg loss: 1.54071\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 107/107 [02:01<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33 avg loss: 1.54030\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 107/107 [02:01<00:00,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34 avg loss: 1.53867\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "n_epochs = 35\n",
    "\n",
    "aux_loss = 1e9\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    losses = []\n",
    "    for batch_graphs, labels, target_nodes, target_edges in tqdm(train_dataloader):\n",
    "        batch_graphs = batch_graphs.to(device)\n",
    "        target_nodes = target_nodes.to(device)\n",
    "        target_edges = target_edges.to(device)\n",
    "        y = labels.to(device)\n",
    "\n",
    "        timesteps = torch.randint(0, 999, (batch_graphs.num_graphs,), device=device)\n",
    "        noise = {\n",
    "            'x': torch.randn((batch_graphs.x.size(0), len(allowed_atoms)), device=device),\n",
    "            'edge_attr': torch.randn_like(batch_graphs.edge_attr)\n",
    "        }\n",
    "\n",
    "        noisy_batch = noise_scheduler.add_noise(batch_graphs, noise, timesteps)\n",
    "        pred = net(noisy_batch, timesteps, y)\n",
    "\n",
    "        loss_node = loss_fn_node(pred['x'], target_nodes)\n",
    "        loss_edge = loss_fn_edge(pred['edge_attr'], target_edges)\n",
    "\n",
    "        val_loss = valence_violation_loss(batch_graphs, pred['x'], pred['edge_attr'])\n",
    "        loss = loss_node + loss_edge +  val_loss\n",
    "\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(net.parameters(), max_norm=1.0)\n",
    "        opt.step()\n",
    "        losses.append(loss.item())\n",
    "    \n",
    "    if aux_loss > loss.item():\n",
    "        torch.save(net, 'graph_diffusion_model.pt')\n",
    "        aux_loss = loss.item()\n",
    "\n",
    "    avg_loss = sum(losses) / len(losses)\n",
    "    print(f\"Epoch {epoch} avg loss: {avg_loss:.5f}\")\n",
    "\n",
    "    # Save best model etc. here as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2267fe29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling:   0%|          | 0/100 [00:00<?, ?it/s]/home/jgduarte/Documents/RA/Projects/3M/PolyGraphPy/.venv/lib/python3.13/site-packages/diffusers/configuration_utils.py:141: FutureWarning: Accessing config attribute `num_train_timesteps` directly via 'GraphDDPMScheduler' object attribute is deprecated. Please access 'num_train_timesteps' over 'GraphDDPMScheduler's config object instead, e.g. 'scheduler.config.num_train_timesteps'.\n",
      "  deprecate(\"direct config name access\", \"1.0.0\", deprecation_message, standard_warn=False)\n",
      "Sampling:   0%|          | 0/100 [00:07<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot convert float NaN to integer",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 42\u001b[39m\n\u001b[32m     40\u001b[39m \u001b[38;5;66;03m# Example usage:\u001b[39;00m\n\u001b[32m     41\u001b[39m net = torch.load(\u001b[33m'\u001b[39m\u001b[33mgraph_diffusion_model.pt\u001b[39m\u001b[33m'\u001b[39m, weights_only=\u001b[38;5;28;01mFalse\u001b[39;00m).to(device)\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m samples = \u001b[43msample_conditioned_molecules\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnoise_scheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_target\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 30\u001b[39m, in \u001b[36msample_conditioned_molecules\u001b[39m\u001b[34m(net, scheduler, y_target, num_samples, device)\u001b[39m\n\u001b[32m     27\u001b[39m         model_output = net(graph, t_tensor, y)\n\u001b[32m     28\u001b[39m         graph = scheduler.step(model_output, t_tensor, graph)\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m mol = \u001b[43mgraph_to_mol\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_valid_molecule(mol):\n\u001b[32m     32\u001b[39m     smiles = Chem.MolToSmiles(mol)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 230\u001b[39m, in \u001b[36mgraph_to_mol\u001b[39m\u001b[34m(data)\u001b[39m\n\u001b[32m    228\u001b[39m added_bonds.add((u, v))\n\u001b[32m    229\u001b[39m bond_val = data.edge_attr[k].item()  \u001b[38;5;66;03m# scalar\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m230\u001b[39m bond_type_idx = \u001b[38;5;28;43mround\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbond_val\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    231\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m bond_type_idx <= \u001b[32m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m bond_type_idx >= \u001b[38;5;28mlen\u001b[39m(allowed_bonds):\n\u001b[32m    232\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[31mValueError\u001b[39m: cannot convert float NaN to integer"
     ]
    }
   ],
   "source": [
    "# Sampling conditioned on scaled polarizability\n",
    "def sample_conditioned_molecules(net, scheduler, y_target, num_samples=100, device=device):\n",
    "    net.eval()\n",
    "    scheduler.alphas_cumprod = scheduler.alphas_cumprod.to(device)\n",
    "    valid_smiles = []\n",
    "    # Load training dataset to sample graphs\n",
    "    dataset = CustomDataset('../polygraphpy/data/polarizability_data.csv')\n",
    "\n",
    "    for i in tqdm(range(num_samples), desc=\"Sampling\"):\n",
    "        # Sample a random training graph structure\n",
    "        idx = torch.randint(0, len(dataset), (1,)).item()\n",
    "        graph, _, _, _ = dataset[idx]\n",
    "        graph = graph.to(device)\n",
    "        # Fully noise it (start from t=999)\n",
    "        timesteps = torch.full((1,), scheduler.num_train_timesteps - 1, device=device)\n",
    "        noise = {\n",
    "            'x': torch.randn((graph.num_nodes, len(allowed_atoms)), device=device),\n",
    "            'edge_attr': torch.randn_like(graph.edge_attr)\n",
    "        }\n",
    "        graph = scheduler.add_noise(graph, noise, timesteps)\n",
    "        # Set condition\n",
    "        y = torch.tensor([y_target], dtype=torch.float32, device=device)\n",
    "\n",
    "        for t in reversed(range(scheduler.num_train_timesteps)):\n",
    "            t_tensor = torch.tensor([t], device=device)\n",
    "            with torch.no_grad():\n",
    "                model_output = net(graph, t_tensor, y)\n",
    "                graph = scheduler.step(model_output, t_tensor, graph)\n",
    "\n",
    "        mol = graph_to_mol(graph)\n",
    "        if is_valid_molecule(mol):\n",
    "            smiles = Chem.MolToSmiles(mol)\n",
    "            valid_smiles.append(smiles)\n",
    "            print(f\"[✓] Sample {i}: {smiles}\")\n",
    "        else:\n",
    "            print(f\"[✗] Sample {i} invalid.\")\n",
    "\n",
    "    return valid_smiles\n",
    "\n",
    "# Example usage:\n",
    "net = torch.load('graph_diffusion_model.pt', weights_only=False).to(device)\n",
    "samples = sample_conditioned_molecules(net, noise_scheduler, y_target=0.0, num_samples=100, device=device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
