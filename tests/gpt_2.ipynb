{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88572c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import re # Import regex for parsing PDB REMARK line\n",
    "\n",
    "# Define a vocabulary for atom types and coordinate bins\n",
    "ATOM_VOCAB = {\n",
    "    'PAD': 0, 'START': 1, 'END': 2,\n",
    "    'C': 3, 'O': 4, 'N': 5, 'H': 6,\n",
    "    'S': 7, 'F': 8, 'Cl': 9, 'Br': 10, 'I': 11 # Add more common atoms as needed\n",
    "}\n",
    "\n",
    "REV_ATOM_VOCAB = {v: k for k, v in ATOM_VOCAB.items()}\n",
    "\n",
    "COORD_MIN = -25.0\n",
    "COORD_MAX = 25.0\n",
    "NUM_COORD_BINS = 50 # Increased bins for better coordinate resolution\n",
    "COORD_BIN_SIZE = (COORD_MAX - COORD_MIN) / NUM_COORD_BINS\n",
    "COORD_TOKEN_OFFSET = len(ATOM_VOCAB)\n",
    "TOTAL_VOCAB_SIZE = COORD_TOKEN_OFFSET + (NUM_COORD_BINS * 3) # Max bin index for coords\n",
    "\n",
    "MAX_ATOMS_IN_DATASET = 200 # This should be determined by the largest molecule in your ~9000 PDBs.\n",
    "\n",
    "def discretize_coord(coord_val):\n",
    "    \"\"\"Discretizes a single coordinate value into a bin index.\"\"\"\n",
    "    bin_idx = int((coord_val - COORD_MIN) / COORD_BIN_SIZE)\n",
    "    return max(0, min(NUM_COORD_BINS - 1, bin_idx))\n",
    "\n",
    "def parse_pdb_content_and_generate_dataset(pdb_content, num_molecules_to_generate=1000, coord_noise_std=0.1, pol_noise_std=0.5):\n",
    "    base_atoms_data = []\n",
    "    base_polarizability = None\n",
    "    base_connectivity = set() # Store as a set of tuples (atom_idx1, atom_idx2) for uniqueness\n",
    "\n",
    "    # Parse polarizability from REMARK line\n",
    "    polarizability_match = re.search(r\"REMARK static_polarizability ([\\d.]+)\", pdb_content)\n",
    "    if polarizability_match:\n",
    "        base_polarizability = float(polarizability_match.group(1))\n",
    "    else:\n",
    "        print(\"Warning: 'static_polarizability' not found in PDB REMARK. Using a default value of 0.\")\n",
    "        base_polarizability = 0 # Default if not found\n",
    "\n",
    "    # Parse HETATM lines to get atom types and coordinates\n",
    "    atom_pdb_id_to_idx = {} # Map PDB atom ID (1-indexed) to 0-indexed list index\n",
    "    current_atom_idx = 0\n",
    "    for line in pdb_content.splitlines():\n",
    "        if line.startswith(\"HETATM\"):\n",
    "            try:\n",
    "                pdb_atom_id = int(line[6:11].strip()) # PDB atom serial number\n",
    "                atom_type = line[76:78].strip() # Element symbol\n",
    "                x = float(line[30:38])\n",
    "                y = float(line[38:46])\n",
    "                z = float(line[46:54])\n",
    "\n",
    "                if atom_type not in ATOM_VOCAB:\n",
    "                    # Add new atom types to vocabulary if encountered\n",
    "                    print(f\"Adding new atom type '{atom_type}' to vocabulary.\")\n",
    "                    ATOM_VOCAB[atom_type] = len(ATOM_VOCAB)\n",
    "                    REV_ATOM_VOCAB[ATOM_VOCAB[atom_type]] = atom_type\n",
    "                    # Recompute COORD_TOKEN_OFFSET and TOTAL_VOCAB_SIZE\n",
    "                    global COORD_TOKEN_OFFSET, TOTAL_VOCAB_SIZE\n",
    "                    COORD_TOKEN_OFFSET = len(ATOM_VOCAB)\n",
    "                    TOTAL_VOCAB_SIZE = COORD_TOKEN_OFFSET + (NUM_COORD_BINS * 3)\n",
    "\n",
    "                base_atoms_data.append({\n",
    "                    'type': atom_type,\n",
    "                    'coords': (x, y, z)\n",
    "                })\n",
    "                atom_pdb_id_to_idx[pdb_atom_id] = current_atom_idx\n",
    "                current_atom_idx += 1\n",
    "            except ValueError as e:\n",
    "                print(f\"Error parsing HETATM line: {line} - {e}\")\n",
    "                continue # Skip malformed lines\n",
    "\n",
    "    # Parse CONECT lines to get connectivity\n",
    "    for line in pdb_content.splitlines():\n",
    "        if line.startswith(\"CONECT\"):\n",
    "            try:\n",
    "                # PDB CONECT format: CONECT atom1 atom2 atom3 ...\n",
    "                # All IDs are 1-indexed\n",
    "                connected_ids = [int(line[i:i+5].strip()) for i in range(6, len(line), 5) if line[i:i+5].strip()]\n",
    "                if not connected_ids:\n",
    "                    continue\n",
    "\n",
    "                atom1_pdb_id = connected_ids[0]\n",
    "                if atom1_pdb_id not in atom_pdb_id_to_idx:\n",
    "                    print(f\"Warning: Atom ID {atom1_pdb_id} in CONECT not found in HETATM records. Skipping.\")\n",
    "                    continue\n",
    "                atom1_idx = atom_pdb_id_to_idx[atom1_pdb_id]\n",
    "\n",
    "                for atom2_pdb_id in connected_ids[1:]:\n",
    "                    if atom2_pdb_id not in atom_pdb_id_to_idx:\n",
    "                        print(f\"Warning: Atom ID {atom2_pdb_id} in CONECT not found in HETATM records. Skipping.\")\n",
    "                        continue\n",
    "                    atom2_idx = atom_pdb_id_to_idx[atom2_pdb_id]\n",
    "                    # Ensure consistent order for bond pairs (smaller index first)\n",
    "                    bond = tuple(sorted((atom1_idx, atom2_idx)))\n",
    "                    if bond[0] != bond[1]: # Avoid self-loops\n",
    "                        base_connectivity.add(bond)\n",
    "            except ValueError as e:\n",
    "                print(f\"Error parsing CONECT line: {line} - {e}\")\n",
    "                continue\n",
    "\n",
    "    if not base_atoms_data:\n",
    "        raise ValueError(\"No HETATM records found in the provided PDB content.\")\n",
    "\n",
    "    num_base_atoms = len(base_atoms_data)\n",
    "    # Ensure MAX_ATOMS_IN_DATASET is large enough\n",
    "    if num_base_atoms > MAX_ATOMS_IN_DATASET:\n",
    "        raise ValueError(f\"Base molecule has {num_base_atoms} atoms, but MAX_ATOMS_IN_DATASET is {MAX_ATOMS_IN_DATASET}. Please increase MAX_ATOMS_IN_DATASET.\")\n",
    "\n",
    "    # Generate augmented dataset\n",
    "    data = []\n",
    "    max_seq_len_overall = 0\n",
    "\n",
    "    for i in range(num_molecules_to_generate):\n",
    "        mol_tokens = [ATOM_VOCAB['START']]\n",
    "        # Add noise to polarizability\n",
    "        current_polarizability = base_polarizability + random.gauss(0, pol_noise_std)\n",
    "        current_polarizability = max(1.0, current_polarizability) # Ensure positive\n",
    "\n",
    "        # Store noisy coordinates to reconstruct connectivity matrix for this augmented molecule\n",
    "        noisy_coords_for_connectivity = []\n",
    "\n",
    "        for atom_info in base_atoms_data:\n",
    "            atom_type = atom_info['type']\n",
    "            x, y, z = atom_info['coords']\n",
    "\n",
    "            # Add Gaussian noise to coordinates\n",
    "            x_noisy = x + random.gauss(0, coord_noise_std)\n",
    "            y_noisy = y + random.gauss(0, coord_noise_std)\n",
    "            z_noisy = z + random.gauss(0, coord_noise_std)\n",
    "\n",
    "            mol_tokens.append(ATOM_VOCAB[atom_type])\n",
    "            mol_tokens.append(discretize_coord(x_noisy) + COORD_TOKEN_OFFSET)\n",
    "            mol_tokens.append(discretize_coord(y_noisy) + COORD_TOKEN_OFFSET + NUM_COORD_BINS) # Offset for Y bins\n",
    "            mol_tokens.append(discretize_coord(z_noisy) + COORD_TOKEN_OFFSET + (NUM_COORD_BINS * 2)) # Offset for Z bins\n",
    "            noisy_coords_for_connectivity.append((x_noisy, y_noisy, z_noisy))\n",
    "\n",
    "        mol_tokens.append(ATOM_VOCAB['END'])\n",
    "\n",
    "        current_seq_len = len(mol_tokens)\n",
    "        max_seq_len_overall = max(max_seq_len_overall, current_seq_len)\n",
    "\n",
    "        # Create the target connectivity matrix for this augmented molecule\n",
    "        # We'll use a flattened upper triangular matrix representation\n",
    "        # Size of flattened upper triangular matrix for N atoms is N * (N - 1) / 2\n",
    "        num_possible_bonds = MAX_ATOMS_IN_DATASET * (MAX_ATOMS_IN_DATASET - 1) // 2\n",
    "        connectivity_target = torch.zeros(num_possible_bonds, dtype=torch.float)\n",
    "\n",
    "        # Populate the connectivity target based on base_connectivity\n",
    "        # Only consider bonds between atoms present in the base molecule\n",
    "        bond_idx = 0\n",
    "        for i in range(MAX_ATOMS_IN_DATASET):\n",
    "            for j in range(i + 1, MAX_ATOMS_IN_DATASET):\n",
    "                if i < num_base_atoms and j < num_base_atoms: # Only for atoms actually in this molecule\n",
    "                    if (i, j) in base_connectivity:\n",
    "                        connectivity_target[bond_idx] = 1.0\n",
    "                bond_idx += 1\n",
    "\n",
    "        data.append({\n",
    "            'tokens': torch.tensor(mol_tokens, dtype=torch.long),\n",
    "            'polarizability': torch.tensor(current_polarizability, dtype=torch.float),\n",
    "            'connectivity_matrix': connectivity_target # This is the flattened upper triangular matrix\n",
    "        })\n",
    "\n",
    "    # After collecting all data, pad them to the maximum sequence length found\n",
    "    padded_data = []\n",
    "    for item in data:\n",
    "        padded_mol_tokens = item['tokens'].tolist() + [ATOM_VOCAB['PAD']] * (max_seq_len_overall - len(item['tokens']))\n",
    "        padded_data.append({\n",
    "            'tokens': torch.tensor(padded_mol_tokens, dtype=torch.long),\n",
    "            'polarizability': item['polarizability'],\n",
    "            'connectivity_matrix': item['connectivity_matrix']\n",
    "        })\n",
    "\n",
    "    return padded_data, max_seq_len_overall, num_base_atoms\n",
    "\n",
    "# Custom Dataset class\n",
    "class MoleculeDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]['tokens'], self.data[idx]['polarizability'], self.data[idx]['connectivity_matrix']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "90e49167",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Reading PDB files from 'xyz_files'...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 677\u001b[39m\n\u001b[32m    673\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPredicted Bonds: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join(predicted_bonds)\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mif\u001b[39;00m\u001b[38;5;250m \u001b[39mpredicted_bonds\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01melse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[33m'\u001b[39m\u001b[33mNone\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    676\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m677\u001b[39m     \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 511\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    508\u001b[39m num_parsed_pdbs = \u001b[32m0\u001b[39m\n\u001b[32m    510\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mReading PDB files from \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpdb_folder\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m511\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m filename \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpdb_folder\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    512\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m filename.startswith(\u001b[33m'\u001b[39m\u001b[33mmonomer_\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m filename.endswith(\u001b[33m'\u001b[39m\u001b[33m.pdb\u001b[39m\u001b[33m'\u001b[39m):\n\u001b[32m    513\u001b[39m         filepath = os.path.join(pdb_folder, filename)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import re # Import regex for parsing PDB REMARK line\n",
    "import os # Import the os module for file system operations\n",
    "\n",
    "# --- 1. Data Generation and Preprocessing (PDB Parsing and Data Augmentation) ---\n",
    "\n",
    "# Define a vocabulary for atom types and coordinate bins\n",
    "ATOM_VOCAB = {\n",
    "    'PAD': 0, 'START': 1, 'END': 2,\n",
    "    'C': 3, 'O': 4, 'N': 5, 'H': 6,\n",
    "    'S': 7, 'F': 8, 'Cl': 9, 'Br': 10, 'I': 11 # Add more common atoms as needed\n",
    "}\n",
    "# Reverse mapping for decoding\n",
    "REV_ATOM_VOCAB = {v: k for k, v in ATOM_VOCAB.items()}\n",
    "\n",
    "# Coordinate binning parameters\n",
    "COORD_MIN = -15.0\n",
    "COORD_MAX = 15.0\n",
    "NUM_COORD_BINS = 40 # Increased bins for better coordinate resolution\n",
    "COORD_BIN_SIZE = (COORD_MAX - COORD_MIN) / NUM_COORD_BINS\n",
    "\n",
    "# These will be updated dynamically based on the parsed data\n",
    "# Initialize with placeholder values. Actual values will be determined during data loading.\n",
    "COORD_TOKEN_OFFSET = len(ATOM_VOCAB)\n",
    "TOTAL_VOCAB_SIZE = COORD_TOKEN_OFFSET + (NUM_COORD_BINS * 3)\n",
    "\n",
    "def update_vocab_and_offsets(atom_type):\n",
    "    \"\"\"Dynamically updates ATOM_VOCAB and recalculates global offsets.\"\"\"\n",
    "    global ATOM_VOCAB, REV_ATOM_VOCAB, COORD_TOKEN_OFFSET, TOTAL_VOCAB_SIZE\n",
    "    if atom_type not in ATOM_VOCAB:\n",
    "        print(f\"Adding new atom type '{atom_type}' to vocabulary.\")\n",
    "        ATOM_VOCAB[atom_type] = len(ATOM_VOCAB)\n",
    "        REV_ATOM_VOCAB[ATOM_VOCAB[atom_type]] = atom_type\n",
    "        # Recalculate offsets and total vocab size\n",
    "        COORD_TOKEN_OFFSET = len(ATOM_VOCAB)\n",
    "        TOTAL_VOCAB_SIZE = COORD_TOKEN_OFFSET + (NUM_COORD_BINS * 3)\n",
    "\n",
    "\n",
    "def discretize_coord(coord_val):\n",
    "    \"\"\"Discretizes a single coordinate value into a bin index.\"\"\"\n",
    "    bin_idx = int((coord_val - COORD_MIN) / COORD_BIN_SIZE)\n",
    "    return max(0, min(NUM_COORD_BINS - 1, bin_idx))\n",
    "\n",
    "def parse_pdb_content_and_generate_dataset(pdb_content, num_molecules_to_augment_per_pdb=1, coord_noise_std=0.1, pol_noise_std=0.5):\n",
    "    \"\"\"\n",
    "    Parses a single PDB content string, extracts atom types, coordinates, polarizability,\n",
    "    and connectivity. Then, generates `num_molecules_to_augment_per_pdb` perturbed versions\n",
    "    of this molecule.\n",
    "\n",
    "    Args:\n",
    "        pdb_content (str): The content of a single PDB file.\n",
    "        num_molecules_to_augment_per_pdb (int): The number of augmented versions to create from this single PDB.\n",
    "        coord_noise_std (float): Standard deviation of Gaussian noise added to coordinates.\n",
    "        pol_noise_std (float): Standard deviation of Gaussian noise added to polarizability.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - list: List of dictionaries, each with 'tokens', 'polarizability', and 'connectivity_matrix'.\n",
    "            - int: Maximum sequence length observed for this molecule's augmentations.\n",
    "            - int: Number of atoms in the base molecule.\n",
    "            - float: The base polarizability from the PDB.\n",
    "    \"\"\"\n",
    "    base_atoms_data = []\n",
    "    base_polarizability = None\n",
    "    base_connectivity = set() # Store as a set of tuples (atom_idx1, atom_idx2) for uniqueness\n",
    "\n",
    "    # Parse polarizability from REMARK line\n",
    "    polarizability_match = re.search(r\"REMARK static_polarizability ([\\d.]+)\", pdb_content)\n",
    "    if polarizability_match:\n",
    "        base_polarizability = float(polarizability_match.group(1))\n",
    "    else:\n",
    "        # Fallback if polarizability not found in PDB (adjust as per your data)\n",
    "        # For a full dataset, you might want to exclude files without this REMARK\n",
    "        print(\"Warning: 'static_polarizability' not found in PDB REMARK. Using a default value of 10.0.\")\n",
    "        base_polarizability = 10.0\n",
    "\n",
    "    # Parse HETATM lines to get atom types and coordinates\n",
    "    atom_pdb_id_to_idx = {} # Map PDB atom ID (1-indexed) to 0-indexed list index\n",
    "    current_atom_idx = 0\n",
    "    for line in pdb_content.splitlines():\n",
    "        if line.startswith(\"HETATM\"):\n",
    "            try:\n",
    "                pdb_atom_id = int(line[6:11].strip()) # PDB atom serial number\n",
    "                atom_type = line[76:78].strip() # Element symbol\n",
    "\n",
    "                # Dynamically update vocabulary for new atom types\n",
    "                update_vocab_and_offsets(atom_type)\n",
    "\n",
    "                x = float(line[30:38])\n",
    "                y = float(line[38:46])\n",
    "                z = float(line[46:54])\n",
    "\n",
    "                base_atoms_data.append({\n",
    "                    'type': atom_type,\n",
    "                    'coords': (x, y, z)\n",
    "                })\n",
    "                atom_pdb_id_to_idx[pdb_atom_id] = current_atom_idx\n",
    "                current_atom_idx += 1\n",
    "            except ValueError as e:\n",
    "                print(f\"Error parsing HETATM line: '{line}' - {e}. Skipping.\")\n",
    "                continue # Skip malformed lines\n",
    "\n",
    "    # Parse CONECT lines to get connectivity\n",
    "    for line in pdb_content.splitlines():\n",
    "        if line.startswith(\"CONECT\"):\n",
    "            try:\n",
    "                connected_ids = [int(line[i:i+5].strip()) for i in range(6, len(line), 5) if line[i:i+5].strip()]\n",
    "                if not connected_ids:\n",
    "                    continue\n",
    "\n",
    "                atom1_pdb_id = connected_ids[0]\n",
    "                if atom1_pdb_id not in atom_pdb_id_to_idx:\n",
    "                    print(f\"Warning: Atom ID {atom1_pdb_id} in CONECT not found in HETATM records. Skipping bond.\")\n",
    "                    continue\n",
    "                atom1_idx = atom_pdb_id_to_idx[atom1_pdb_id]\n",
    "\n",
    "                for atom2_pdb_id in connected_ids[1:]:\n",
    "                    if atom2_pdb_id not in atom_pdb_id_to_idx:\n",
    "                        print(f\"Warning: Atom ID {atom2_pdb_id} in CONECT not found in HETATM records. Skipping bond.\")\n",
    "                        continue\n",
    "                    atom2_idx = atom_pdb_id_to_idx[atom2_pdb_id]\n",
    "                    # Ensure consistent order for bond pairs (smaller index first)\n",
    "                    bond = tuple(sorted((atom1_idx, atom2_idx)))\n",
    "                    if bond[0] != bond[1]: # Avoid self-loops\n",
    "                        base_connectivity.add(bond)\n",
    "            except ValueError as e:\n",
    "                print(f\"Error parsing CONECT line: '{line}' - {e}. Skipping.\")\n",
    "                continue\n",
    "\n",
    "    if not base_atoms_data:\n",
    "        raise ValueError(\"No HETATM records found in the provided PDB content. Cannot generate data.\")\n",
    "\n",
    "    num_base_atoms = len(base_atoms_data)\n",
    "    # The actual `MAX_ATOMS_IN_DATASET` (overall largest molecule) will be determined\n",
    "    # in train_model and passed to the LLM. For individual parsing, we just need `num_base_atoms`.\n",
    "\n",
    "    # Generate augmented dataset\n",
    "    augmented_data = []\n",
    "    max_seq_len_for_this_pdb = 0\n",
    "\n",
    "    for i in range(num_molecules_to_augment_per_pdb):\n",
    "        mol_tokens = [ATOM_VOCAB['START']]\n",
    "        # Add noise to polarizability\n",
    "        current_polarizability = base_polarizability + random.gauss(0, pol_noise_std)\n",
    "        current_polarizability = max(1.0, current_polarizability) # Ensure positive\n",
    "\n",
    "        for atom_info in base_atoms_data:\n",
    "            atom_type = atom_info['type']\n",
    "            x, y, z = atom_info['coords']\n",
    "\n",
    "            # Add Gaussian noise to coordinates\n",
    "            x_noisy = x + random.gauss(0, coord_noise_std)\n",
    "            y_noisy = y + random.gauss(0, coord_noise_std)\n",
    "            z_noisy = z + random.gauss(0, coord_noise_std)\n",
    "\n",
    "            mol_tokens.append(ATOM_VOCAB[atom_type])\n",
    "            mol_tokens.append(discretize_coord(x_noisy) + COORD_TOKEN_OFFSET)\n",
    "            mol_tokens.append(discretize_coord(y_noisy) + COORD_TOKEN_OFFSET + NUM_COORD_BINS)\n",
    "            mol_tokens.append(discretize_coord(z_noisy) + COORD_TOKEN_OFFSET + (NUM_COORD_BINS * 2))\n",
    "\n",
    "        mol_tokens.append(ATOM_VOCAB['END'])\n",
    "\n",
    "        current_seq_len = len(mol_tokens)\n",
    "        max_seq_len_for_this_pdb = max(max_seq_len_for_this_pdb, current_seq_len)\n",
    "\n",
    "        # For augmented data, bonds remain the same as the base molecule\n",
    "        # Connectivity matrix will be created later, after determining global MAX_ATOMS_IN_DATASET\n",
    "        # Store base_connectivity for now.\n",
    "        augmented_data.append({\n",
    "            'tokens': torch.tensor(mol_tokens, dtype=torch.long),\n",
    "            'polarizability': torch.tensor(current_polarizability, dtype=torch.float),\n",
    "            'base_connectivity': base_connectivity, # Store the original set of bonds\n",
    "            'num_base_atoms': num_base_atoms # Store the count for this specific molecule\n",
    "        })\n",
    "\n",
    "    return augmented_data, max_seq_len_for_this_pdb, num_base_atoms, base_polarizability\n",
    "\n",
    "# Custom Dataset class\n",
    "class MoleculeDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]['tokens'], self.data[idx]['polarizability'], self.data[idx]['connectivity_matrix']\n",
    "\n",
    "# --- 2. Property Embedding ---\n",
    "class GaussianExpansion(nn.Module):\n",
    "    \"\"\"\n",
    "    Applies Gaussian expansion to a scalar property.\n",
    "    Transforms a single scalar value into a higher-dimensional vector representation.\n",
    "    \"\"\"\n",
    "    def __init__(self, start=0.0, stop=200.0, num_gaussians=100): # Adjusted default range for polarizability\n",
    "        super().__init__()\n",
    "        self.register_buffer('offset', torch.linspace(start, stop, num_gaussians))\n",
    "        self.register_buffer('widths', torch.tensor((stop - start) / num_gaussians, dtype=torch.float).expand_as(self.offset))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (batch_size, 1) or (batch_size,) tensor of scalar property values\n",
    "        Output: (batch_size, num_gaussians)\n",
    "        \"\"\"\n",
    "        if x.dim() == 1:\n",
    "            x = x.unsqueeze(-1) # Ensure x is (batch_size, 1)\n",
    "        return torch.exp(-((x - self.offset) ** 2) / (2 * self.widths ** 2))\n",
    "\n",
    "class PropertyEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    Combines Gaussian expansion with an MLP to create a high-dimensional property embedding.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_gaussians, hidden_dim, embedding_dim, prop_min_val, prop_max_val):\n",
    "        super().__init__()\n",
    "        # Adjusted GaussianExpansion range\n",
    "        self.gaussian_expansion = GaussianExpansion(start=prop_min_val, stop=prop_max_val, num_gaussians=num_gaussians)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(num_gaussians, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, embedding_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, polarizability):\n",
    "        \"\"\"\n",
    "        polarizability: (batch_size,) tensor of polarizability values\n",
    "        Output: (batch_size, embedding_dim)\n",
    "        \"\"\"\n",
    "        gaussian_features = self.gaussian_expansion(polarizability)\n",
    "        property_embed = self.mlp(gaussian_features)\n",
    "        return property_embed\n",
    "\n",
    "# --- 3. Transformer-based Generative Model ---\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    Injects positional information into token embeddings.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1) # (max_len, 1, d_model)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (seq_len, batch_size, d_model)\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return x\n",
    "\n",
    "class MoleculeGeneratorLLM(nn.Module):\n",
    "    \"\"\"\n",
    "    A simplified Transformer Decoder-only model for molecule generation,\n",
    "    conditioned on a property embedding, and predicting connectivity.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, d_model, nhead, num_decoder_layers,\n",
    "                 dim_feedforward, dropout, max_seq_len,\n",
    "                 num_gaussians_prop, prop_hidden_dim, prop_embedding_dim,\n",
    "                 max_atoms_in_dataset_overall, prop_min_val, prop_max_val):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.vocab_size = vocab_size\n",
    "        self.max_atoms_in_dataset_overall = max_atoms_in_dataset_overall\n",
    "        # Calculate the size of the flattened upper triangular matrix for connectivity\n",
    "        self.num_possible_bonds = max_atoms_in_dataset_overall * (max_atoms_in_dataset_overall - 1) // 2\n",
    "\n",
    "        # Property embedding module\n",
    "        self.property_embedder = PropertyEmbedding(\n",
    "            num_gaussians=num_gaussians_prop,\n",
    "            hidden_dim=prop_hidden_dim,\n",
    "            embedding_dim=prop_embedding_dim,\n",
    "            prop_min_val=prop_min_val,\n",
    "            prop_max_val=prop_max_val\n",
    "        )\n",
    "\n",
    "        # Token embedding layer\n",
    "        self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_seq_len)\n",
    "\n",
    "        # Linear layer to project property embedding to d_model for concatenation/addition\n",
    "        self.prop_proj = nn.Linear(prop_embedding_dim, d_model)\n",
    "\n",
    "        # Transformer Decoder\n",
    "        # Ensure batch_first=True is consistently applied\n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model, nhead, dim_feedforward, dropout, batch_first=True)\n",
    "        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_decoder_layers)\n",
    "\n",
    "        # Output layer for token prediction\n",
    "        self.token_output_layer = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "        # New: Output head for connectivity prediction\n",
    "        self.bond_prediction_head = nn.Sequential(\n",
    "            nn.Linear(d_model, dim_feedforward), # Use d_model as input since we'll pool\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(dim_feedforward, self.num_possible_bonds)\n",
    "        )\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        \"\"\"Initializes weights for better training stability.\"\"\"\n",
    "        initrange = 0.1\n",
    "        self.token_embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.token_output_layer.bias.data.zero_()\n",
    "        self.token_output_layer.weight.data.uniform_(-initrange, initrange)\n",
    "        # Initialize bond prediction head weights\n",
    "        for layer in self.bond_prediction_head:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                layer.bias.data.zero_()\n",
    "                layer.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def generate_square_subsequent_mask(self, sz):\n",
    "        \"\"\"Generates an upper-triangular matrix of -inf, used for masking future tokens.\"\"\"\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "\n",
    "    def forward(self, src_tokens, polarizability):\n",
    "        \"\"\"\n",
    "        src_tokens: (batch_size, seq_len) - input sequence (e.g., [START_TOKEN, atom1, x1, y1, z1, ...])\n",
    "        polarizability: (batch_size,) - desired polarizability for each molecule\n",
    "        \"\"\"\n",
    "        batch_size, seq_len = src_tokens.shape\n",
    "\n",
    "        # 1. Embed property\n",
    "        prop_embed = self.property_embedder(polarizability) # (batch_size, prop_embedding_dim)\n",
    "\n",
    "        # 2. Embed input tokens\n",
    "        token_embed = self.token_embedding(src_tokens) * math.sqrt(self.d_model) # (batch_size, seq_len, d_model)\n",
    "        # Positional encoding expects (seq_len, batch_size, d_model), so transpose for PE, then transpose back\n",
    "        token_embed = self.positional_encoding(token_embed.transpose(0, 1)).transpose(0, 1) # (batch_size, seq_len, d_model)\n",
    "\n",
    "        # TransformerDecoder forward pass\n",
    "        # tgt should be (batch_size, seq_len, d_model) because batch_first=True\n",
    "        # memory should be (batch_size, memory_len, d_model) because batch_first=True\n",
    "        decoder_output = self.transformer_decoder(\n",
    "            tgt=token_embed, # (batch_size, seq_len, d_model) - NO TRANSPOSE HERE\n",
    "            memory=self.prop_proj(prop_embed).unsqueeze(1), # (batch_size, 1, d_model) - Corrected memory shape\n",
    "            tgt_mask=self.generate_square_subsequent_mask(seq_len).to(src_tokens.device), # Mask is (seq_len, seq_len)\n",
    "            tgt_key_padding_mask=(src_tokens == ATOM_VOCAB['PAD']),\n",
    "        )\n",
    "        # decoder_output is already (batch_size, seq_len, d_model) because batch_first=True\n",
    "        # No need for decoder_output.transpose(0, 1) here\n",
    "\n",
    "        # Predict tokens\n",
    "        token_logits = self.token_output_layer(decoder_output) # (batch_size, seq_len, vocab_size)\n",
    "\n",
    "        # Predict connectivity: Use the mean of the decoder output across the sequence length\n",
    "        pooled_decoder_output = decoder_output.mean(dim=1) # (batch_size, d_model)\n",
    "        connectivity_logits = self.bond_prediction_head(pooled_decoder_output) # (batch_size, num_possible_bonds)\n",
    "\n",
    "        return token_logits, connectivity_logits\n",
    "\n",
    "    def generate(self, polarizability, max_new_tokens=50, temperature=1.0, top_k=None, top_p=None):\n",
    "        \"\"\"\n",
    "        Generates a new molecule sequence and predicts its connectivity\n",
    "        given a desired polarizability.\n",
    "        \"\"\"\n",
    "        self.eval() # Set model to evaluation mode\n",
    "        device = next(self.parameters()).device\n",
    "\n",
    "        # Prepare polarizability input\n",
    "        polarizability_tensor = torch.tensor([polarizability], dtype=torch.float, device=device)\n",
    "\n",
    "        # Initialize sequence with START token\n",
    "        generated_sequence = [ATOM_VOCAB['START']]\n",
    "        input_tokens = torch.tensor(generated_sequence, dtype=torch.long, device=device).unsqueeze(0) # (1, 1)\n",
    "\n",
    "        for _ in range(max_new_tokens):\n",
    "            if input_tokens.shape[1] >= self.max_seq_len:\n",
    "                break # Avoid exceeding max sequence length\n",
    "\n",
    "            with torch.no_grad():\n",
    "                token_logits, _ = self.forward(input_tokens, polarizability_tensor)\n",
    "\n",
    "            next_token_logits = token_logits[:, -1, :] # (1, vocab_size)\n",
    "\n",
    "            # Apply sampling strategies\n",
    "            if temperature != 1.0:\n",
    "                next_token_logits = next_token_logits / temperature\n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(next_token_logits, min(top_k, next_token_logits.size(-1)))\n",
    "                next_token_logits[next_token_logits < v[:, [-1]]] = -float('Inf')\n",
    "            if top_p is not None:\n",
    "                sorted_logits, sorted_indices = torch.sort(next_token_logits, descending=True)\n",
    "                cumulative_probs = torch.cumsum(torch.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "                sorted_indices_to_remove = cumulative_probs > top_p\n",
    "                sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "                sorted_indices_to_remove[..., 0] = 0\n",
    "                next_token_logits = next_token_logits.scatter_(1, sorted_indices[sorted_indices_to_remove], float('-Inf'))\n",
    "\n",
    "            # Sample the next token\n",
    "            probs = torch.softmax(next_token_logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1).squeeze(0) # (1,)\n",
    "\n",
    "            generated_sequence.append(next_token.item())\n",
    "\n",
    "            if next_token.item() == ATOM_VOCAB['END']:\n",
    "                break\n",
    "\n",
    "            input_tokens = torch.cat([input_tokens, next_token.unsqueeze(0)], dim=1)\n",
    "\n",
    "        # After generating the full sequence, make a final forward pass to get connectivity prediction\n",
    "        final_input_tokens = torch.tensor(generated_sequence, dtype=torch.long, device=device).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            _, connectivity_logits = self.forward(final_input_tokens, polarizability_tensor)\n",
    "\n",
    "        predicted_bonds_flat_thresholded = (torch.sigmoid(connectivity_logits) > 0.5).squeeze(0).cpu().numpy()\n",
    "\n",
    "        # Decode the generated sequence\n",
    "        decoded_molecule_str = []\n",
    "        atoms_generated_count = 0\n",
    "        current_atom_type = None\n",
    "        current_coords_buffer = [] # Use a buffer to store coordinates temporarily\n",
    "\n",
    "        for token_id in generated_sequence:\n",
    "            if token_id == ATOM_VOCAB['START']:\n",
    "                decoded_molecule_str.append(\"START\")\n",
    "            elif token_id == ATOM_VOCAB['END']:\n",
    "                decoded_molecule_str.append(\"END\")\n",
    "                break # Stop decoding after END token\n",
    "            elif token_id == ATOM_VOCAB['PAD']:\n",
    "                continue\n",
    "            elif token_id < COORD_TOKEN_OFFSET: # It's an atom type\n",
    "                if current_atom_type is not None: # If previous atom had incomplete coords\n",
    "                    decoded_molecule_str.append(f\"Atom {atoms_generated_count}: {current_atom_type}, Coords: (Incomplete)\")\n",
    "                    atoms_generated_count += 1\n",
    "                current_atom_type = REV_ATOM_VOCAB.get(token_id, f\"UNKNOWN_ATOM_{token_id}\")\n",
    "                current_coords_buffer = [] # Reset buffer for new atom\n",
    "            elif token_id >= COORD_TOKEN_OFFSET: # It's a coordinate bin\n",
    "                coord_bin_idx = token_id - COORD_TOKEN_OFFSET\n",
    "                # Determine which dimension (x, y, z)\n",
    "                if coord_bin_idx < NUM_COORD_BINS: # X coord\n",
    "                    current_coords_buffer.append(COORD_MIN + coord_bin_idx * COORD_BIN_SIZE)\n",
    "                elif coord_bin_idx < NUM_COORD_BINS * 2: # Y coord\n",
    "                    current_coords_buffer.append(COORD_MIN + (coord_bin_idx - NUM_COORD_BINS) * COORD_BIN_SIZE)\n",
    "                else: # Z coord\n",
    "                    current_coords_buffer.append(COORD_MIN + (coord_bin_idx - NUM_COORD_BINS * 2) * COORD_BIN_SIZE)\n",
    "\n",
    "                if len(current_coords_buffer) == 3 and current_atom_type is not None:\n",
    "                    decoded_molecule_str.append(f\"Atom {atoms_generated_count}: {current_atom_type}, Coords: ({current_coords_buffer[0]:.2f}, {current_coords_buffer[1]:.2f}, {current_coords_buffer[2]:.2f})\")\n",
    "                    atoms_generated_count += 1\n",
    "                    current_atom_type = None # Reset for next atom\n",
    "                    current_coords_buffer = []\n",
    "\n",
    "        # Decode predicted connectivity based on the number of atoms actually generated\n",
    "        predicted_bonds_list = []\n",
    "        bond_flat_idx = 0\n",
    "        # Iterate over possible atom pairs up to the number of atoms generated *or* max_atoms_in_dataset_overall\n",
    "        # whichever is smaller, to correctly map the flattened matrix.\n",
    "        for i in range(min(atoms_generated_count, self.max_atoms_in_dataset_overall)):\n",
    "            for j in range(i + 1, min(atoms_generated_count, self.max_atoms_in_dataset_overall)):\n",
    "                if bond_flat_idx < len(predicted_bonds_flat_thresholded) and predicted_bonds_flat_thresholded[bond_flat_idx] == 1:\n",
    "                    predicted_bonds_list.append(f\"({i}-{j})\")\n",
    "                bond_flat_idx += 1\n",
    "\n",
    "\n",
    "        return \" \".join(decoded_molecule_str), predicted_bonds_list\n",
    "\n",
    "\n",
    "# --- 4. Training Configuration and Loop ---\n",
    "def train_model():\n",
    "    # Device configuration\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Hyperparameters\n",
    "    NUM_MOLECULES_TO_AUGMENT_PER_PDB = 1 # Set to 1 for now, as you have ~9000 unique PDBs\n",
    "    BATCH_SIZE = 32\n",
    "    NUM_EPOCHS = 50\n",
    "    LEARNING_RATE = 1e-4\n",
    "    BOND_LOSS_WEIGHT = 0.5 # Weight for the connectivity prediction loss\n",
    "    COORD_NOISE_STD = 0.1 # Standard deviation for coordinate noise\n",
    "    POL_NOISE_STD = 0.5 # Standard deviation for polarizability noise\n",
    "\n",
    "    # Model parameters\n",
    "    D_MODEL = 256 # Dimension of embeddings and Transformer layers\n",
    "    NHEAD = 8 # Number of attention heads\n",
    "    NUM_DECODER_LAYERS = 3 # Number of Transformer decoder layers\n",
    "    DIM_FEEDFORWARD = 512 # Dimension of the feedforward network in Transformer\n",
    "    DROPOUT = 0.1\n",
    "\n",
    "    # Property embedding parameters\n",
    "    NUM_GAUSSIANS_PROP = 100\n",
    "    PROP_HIDDEN_DIM = 128\n",
    "    PROP_EMBEDDING_DIM = D_MODEL # Match property embedding dim to d_model for easier integration\n",
    "\n",
    "    # --- Data Loading from Folder ---\n",
    "    pdb_folder = 'xyz_files' # Your folder name\n",
    "    # Create the folder if it doesn't exist (useful for testing or if you create dummy files)\n",
    "    os.makedirs(pdb_folder, exist_ok=True)\n",
    "\n",
    "    all_raw_molecules_data = [] # Stores lists of augmented data from each PDB\n",
    "    max_seq_len_overall_dataset = 0\n",
    "    max_atoms_overall_dataset = 0 # This will be the N in N*(N-1)/2 for bond prediction\n",
    "    min_polarizability_overall = float('inf')\n",
    "    max_polarizability_overall = float('-inf')\n",
    "    num_parsed_pdbs = 0\n",
    "\n",
    "    print(f\"Reading PDB files from '{pdb_folder}'...\")\n",
    "    for filename in os.listdir(pdb_folder):\n",
    "        if filename.startswith('monomer_') and filename.endswith('.pdb'):\n",
    "            filepath = os.path.join(pdb_folder, filename)\n",
    "            print(f\"Parsing {filepath}...\")\n",
    "            try:\n",
    "                with open(filepath, 'r') as f:\n",
    "                    pdb_content = f.read()\n",
    "\n",
    "                # parse_pdb_content_and_generate_dataset returns:\n",
    "                # (augmented_data_list, max_seq_len_for_this_pdb, num_base_atoms, base_polarizability)\n",
    "                augmented_data_from_one_pdb, current_max_seq_len, current_num_atoms, current_polarizability = \\\n",
    "                    parse_pdb_content_and_generate_dataset(\n",
    "                        pdb_content,\n",
    "                        num_molecules_to_augment_per_pdb=NUM_MOLECULES_TO_AUGMENT_PER_PDB,\n",
    "                        coord_noise_std=COORD_NOISE_STD, # Pass the defined constants\n",
    "                        pol_noise_std=POL_NOISE_STD      # Pass the defined constants\n",
    "                    )\n",
    "\n",
    "                all_raw_molecules_data.extend(augmented_data_from_one_pdb)\n",
    "                max_seq_len_overall_dataset = max(max_seq_len_overall_dataset, current_max_seq_len)\n",
    "                max_atoms_overall_dataset = max(max_atoms_overall_dataset, current_num_atoms)\n",
    "                min_polarizability_overall = min(min_polarizability_overall, current_polarizability)\n",
    "                max_polarizability_overall = max(max_polarizability_overall, current_polarizability)\n",
    "                num_parsed_pdbs += 1\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to parse {filepath}: {e}\")\n",
    "                continue\n",
    "\n",
    "    if num_parsed_pdbs == 0:\n",
    "        print(f\"No PDB files found or successfully parsed in '{pdb_folder}'. Please check the folder and file names.\")\n",
    "        print(\"Creating a dummy PDB file for demonstration...\")\n",
    "        # Create a dummy PDB file for demonstration if no files are found\n",
    "        dummy_pdb_content = \"\"\"REMARK static_polarizability 184.75\n",
    "HETATM      1  C   MOL     1        -1.000   0.000   0.000  1.00  0.00           C\n",
    "HETATM      2  O   MOL     1         0.000   1.000   0.000  1.00  0.00           O\n",
    "HETATM      3  H   MOL     1         0.000   0.000   1.000  1.00  0.00           H\n",
    "CONECT      1    2\n",
    "CONECT      2    3\n",
    "END\"\"\"\n",
    "        dummy_filepath = os.path.join(pdb_folder, \"monomer_dummy.pdb\")\n",
    "        with open(dummy_filepath, \"w\") as f:\n",
    "            f.write(dummy_pdb_content)\n",
    "        print(f\"Dummy file '{dummy_filepath}' created. Please rerun the script.\")\n",
    "        return # Exit if no real data to process\n",
    "\n",
    "    print(f\"\\nFinished parsing {num_parsed_pdbs} PDB files.\")\n",
    "    print(f\"Total augmented molecules for training: {len(all_raw_molecules_data)}\")\n",
    "    print(f\"Max sequence length observed: {max_seq_len_overall_dataset}\")\n",
    "    print(f\"Max atoms in any molecule (for bond matrix sizing): {max_atoms_overall_dataset}\")\n",
    "    print(f\"Overall polarizability range: {min_polarizability_overall:.2f} - {max_polarizability_overall:.2f}\")\n",
    "\n",
    "    # Now, process all collected data to pad tokens and create final connectivity matrices\n",
    "    final_dataset_items = []\n",
    "    num_possible_bonds_overall = max_atoms_overall_dataset * (max_atoms_overall_dataset - 1) // 2\n",
    "\n",
    "    for item in all_raw_molecules_data:\n",
    "        # Pad tokens to the global max_seq_len\n",
    "        padded_mol_tokens = item['tokens'].tolist() + \\\n",
    "                            [ATOM_VOCAB['PAD']] * (max_seq_len_overall_dataset - len(item['tokens']))\n",
    "\n",
    "        # Create the fixed-size connectivity matrix for each molecule\n",
    "        connectivity_target_tensor = torch.zeros(num_possible_bonds_overall, dtype=torch.float)\n",
    "        base_connectivity = item['base_connectivity']\n",
    "        num_current_atoms = item['num_base_atoms']\n",
    "\n",
    "        bond_idx = 0\n",
    "        for i in range(max_atoms_overall_dataset): # Iterate up to the largest possible molecule size\n",
    "            for j in range(i + 1, max_atoms_overall_dataset):\n",
    "                if i < num_current_atoms and j < num_current_atoms: # Only for atoms actually present in this molecule\n",
    "                    if (i, j) in base_connectivity:\n",
    "                        connectivity_target_tensor[bond_idx] = 1.0\n",
    "                bond_idx += 1\n",
    "\n",
    "        final_dataset_items.append({\n",
    "            'tokens': torch.tensor(padded_mol_tokens, dtype=torch.long),\n",
    "            'polarizability': item['polarizability'],\n",
    "            'connectivity_matrix': connectivity_target_tensor\n",
    "        })\n",
    "\n",
    "    dataset = MoleculeDataset(final_dataset_items)\n",
    "    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "    # Initialize model\n",
    "    model = MoleculeGeneratorLLM(\n",
    "        vocab_size=TOTAL_VOCAB_SIZE, # Use the dynamically updated TOTAL_VOCAB_SIZE\n",
    "        d_model=D_MODEL,\n",
    "        nhead=NHEAD,\n",
    "        num_decoder_layers=NUM_DECODER_LAYERS,\n",
    "        dim_feedforward=DIM_FEEDFORWARD,\n",
    "        dropout=DROPOUT,\n",
    "        max_seq_len=max_seq_len_overall_dataset, # Use the globally determined max seq length\n",
    "        num_gaussians_prop=NUM_GAUSSIANS_PROP,\n",
    "        prop_hidden_dim=PROP_HIDDEN_DIM,\n",
    "        prop_embedding_dim=PROP_EMBEDDING_DIM,\n",
    "        max_atoms_in_dataset_overall=max_atoms_overall_dataset, # Use the globally determined max atoms\n",
    "        prop_min_val=min_polarizability_overall - POL_NOISE_STD * 3, # Adjust range based on observed data\n",
    "        prop_max_val=max_polarizability_overall + POL_NOISE_STD * 3\n",
    "    ).to(device)\n",
    "\n",
    "    # Loss functions\n",
    "    token_criterion = nn.CrossEntropyLoss(ignore_index=ATOM_VOCAB['PAD'])\n",
    "    connectivity_criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    # Training loop\n",
    "    print(\"Starting training...\")\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        model.train()\n",
    "        total_token_loss = 0\n",
    "        total_connectivity_loss = 0\n",
    "        total_overall_loss = 0\n",
    "\n",
    "        for batch_idx, (tokens, polarizabilities, connectivity_matrices) in enumerate(dataloader):\n",
    "            tokens, polarizabilities, connectivity_matrices = \\\n",
    "                tokens.to(device), polarizabilities.to(device), connectivity_matrices.to(device)\n",
    "\n",
    "            input_seq = tokens[:, :-1]\n",
    "            target_seq = tokens[:, 1:]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            token_logits, connectivity_logits = model(input_seq, polarizabilities)\n",
    "\n",
    "            token_loss = token_criterion(token_logits.reshape(-1, token_logits.size(-1)), target_seq.reshape(-1))\n",
    "            connectivity_loss = connectivity_criterion(connectivity_logits, connectivity_matrices)\n",
    "\n",
    "            overall_loss = token_loss + (BOND_LOSS_WEIGHT * connectivity_loss)\n",
    "            overall_loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            total_token_loss += token_loss.item()\n",
    "            total_connectivity_loss += connectivity_loss.item()\n",
    "            total_overall_loss += overall_loss.item()\n",
    "\n",
    "        avg_token_loss = total_token_loss / len(dataloader)\n",
    "        avg_connectivity_loss = total_connectivity_loss / len(dataloader)\n",
    "        avg_overall_loss = total_overall_loss / len(dataloader)\n",
    "        print(f\"Epoch {epoch+1}/{NUM_EPOCHS}, Overall Loss: {avg_overall_loss:.4f}, \"\n",
    "              f\"Token Loss: {avg_token_loss:.4f}, Connectivity Loss: {avg_connectivity_loss:.4f}\")\n",
    "\n",
    "    print(\"Training complete.\")\n",
    "\n",
    "    # --- 5. Generation Example ---\n",
    "    print(\"\\n--- Generating new molecules ---\")\n",
    "    # Example target polarizability values for generation\n",
    "    # Try values within the range of your dataset's polarizabilities\n",
    "    target_polarizabilities = [\n",
    "        min_polarizability_overall * 1.05, # A bit above min\n",
    "        (min_polarizability_overall + max_polarizability_overall) / 2, # Mid-range\n",
    "        max_polarizability_overall * 0.95 # A bit below max\n",
    "    ]\n",
    "\n",
    "    for target_pol in target_polarizabilities:\n",
    "        print(f\"\\nGenerating molecule with target polarizability: {target_pol:.2f}\")\n",
    "        generated_mol_str, predicted_bonds = model.generate(\n",
    "            polarizability=target_pol,\n",
    "            max_new_tokens=max_seq_len_overall_dataset,\n",
    "            temperature=0.8,\n",
    "            top_k=50\n",
    "        )\n",
    "        print(f\"Generated Sequence: {generated_mol_str}\")\n",
    "        print(f\"Predicted Bonds: {', '.join(predicted_bonds) if predicted_bonds else 'None'}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_model()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
